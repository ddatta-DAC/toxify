#!/usr/bin/env python3

import toxify
from toxify import dfUtil
import os
import sys
import numpy as np
import tensorflow as tf
import random


data_dir = os.path.dirname(dfUtil.__file__)
def getOptionValue(option):
    optionPos = [i for i, j in enumerate(sys.argv) if j == option][0]
    optionValue = sys.argv[optionPos + 1]
    return optionValue
if "-t" in sys.argv:
    core_num = getOptionValue("-t")
if "-db" in sys.argv:
    diamond_db = getOptionValue("-db")


train_only = False
test_only = False
predict_only = False
# train_and_test = False

if "--create-features" in sys.argv:
    fasta_dir = getOptionValue("--create-features")
    snake_file = data_dir+"/data/Snakefile"
    # Raptorx_dir = data_dir+"/data/RaptorX_Property_Fast"
    # if not os.path.isfile(Raptorx_dir+"/build_complete.txt"):
    #     os.system("touch "+Raptorx_dir+"/build_complete.txt;cd "+Raptorx_dir+"/source_code;make;cd ../../")
    # bash_command = "snakemake --snakefile "+snake_file+" -d "+fasta_dir+" --config diamond_db="+diamond_db+" --cores "+core_num + " --use-conda --config raptorx_dir="+Raptorx_dir
    bash_command = "snakemake --snakefile "+snake_file+" -d "+fasta_dir+" --cores "+core_num + " --use-conda"
    print(bash_command)
    os.system(bash_command)

if "--prep-train" in sys.argv:
    # should generate six files from list of "all.combined.csv" with relative fasta_paths
    # two test and two train as fm and tf_csv and flat_csv with header column
    fasta_paths = getOptionValue("--prep-train")
    pos_list = []
    neg_list = []
    with open(fasta_paths) as f:
        for line in f:
            row = line.strip().split()
            currentFile = row[1]
            currentDF = pd.read_csv(currentFile, header=None)
            if row[0] == "pos":
                pos_list.append(currentDF)
            if row[0] == "neg":
                neg_list.append(currentDF)
        (train,test) = splitTrain(pos_list,neg_list)
        train.to_csv('fasta_paths.train.csv', na_rep='0')
        test.to_csv('fasta_paths.test.csv', na_rep='0')

        train.drop('N:feature_0', 1)
        test.drop('N:feature_0', 1)
        train_fm = train.insert(0, '.', range(1, 1 + len(train)))
        test_fm = test.insert(0, '.', range(1, 1 + len(test)))


        train_fm.to_csv(fasta_paths+'.train.fm', na_rep='0',sep='\t')
        test_fm.to_csv(fasta_paths+'.test.fm', na_rep='0',sep='\t')

        with open(fasta_paths+'.train.tf.csv', 'w') as out:
            out.write(str(train.shape[0])+","+str(train.shape[1]-1)+"\n")

        with open(fasta_paths+'.test.tf.csv', 'w') as out:
            out.write(str(test.shape[0])+","+str(test.shape[1]-1)+"\n")

        with open(fasta_paths+'.train.tf.csv', 'a') as f:
            train.to_csv(f, na_rep='0', header=False,index = False)

        with open(fasta_paths+'.test.tf.csv', 'a') as f:
            test.to_csv(f, na_rep='0', header=False,index = False)





elif "-train" in sys.argv or "-test" in sys.argv:

    if "-train" in sys.argv:
        training_data = getOptionValue("-train")
        train_only = True
    if "-test" in sys.argv:
        test_data = getOptionValue("-test")
        test_only = True
    elif "-predict" in sys.argv and "-model" in sys.argv:
        predict_data = getOptionValue("-predict")
        modelDir = getOptionValue("-model")
        predict_only = True

    else:
        print("please provide training data with -train")
        print("please provide test data with -test")
        sys.exit()
else:
    print("DO SOMETHING ELSE")
    sys.exit()


if train_only and test_only:
    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=training_data,
        target_dtype=np.int,
        features_dtype=np.float32)
    with open(training_data) as f:
        for line in f:
            row = line.strip().split(",")
            if len(row) ==2:
                data_shape = int(row[1])
            else:
                break
    print("NUM FEATURES:",data_shape)
    feature_columns = [tf.feature_column.numeric_column("x", shape=[data_shape])]
    print(feature_columns)
    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                      hidden_units=[500,500,500],
                                          n_classes=2,
                                          # dropout=0.02,
                                          model_dir="tmp/"+training_data.replace("train","model"),
                                          optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.0001, l1_regularization_strength=0.001)
                                          )
                                          #Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": np.array(training_set.data)},
        y=np.array(training_set.target),
        num_epochs=1000,
        shuffle=True)
    classifier.train(input_fn=train_input_fn, steps=10000000)

    with open(test_data) as f:
        for line in f:
            row = line.strip().split(",")
            if len(row) ==2:
                data_shape = int(row[1])
            else:
                break
    print("NUM FEATURES:",data_shape)
    feature_columns = [tf.feature_column.numeric_column("x", shape=[data_shape])]
    print(feature_columns)
    # classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
    #                                   hidden_units=[500,500,500],
    #                                       n_classes=2,
    #                                       # dropout=0.02,
    #                                       model_dir="tmp/"+test_data.replace("test","model"),
    #                                       optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01, l1_regularization_strength=0.001)
    #                                       )
    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=test_data,
        # na_value='NaN'
        target_dtype=np.int,
        features_dtype=np.float32)



    test_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": np.array(test_set.data)},
        y=np.array(test_set.target),
        num_epochs=1,
        shuffle=False)
    accuracy_score = classifier.evaluate(input_fn=test_input_fn)["accuracy"]

    print("\nTest Accuracy: {0:f}\n".format(accuracy_score))

# data_shape = training_set.shape[1]
if test_only and not train_only:
    with open(test_data) as f:
        for line in f:
            row = line.strip().split(",")
            if len(row) ==2:
                data_shape = int(row[1])
            else:
                break
    print("NUM FEATURES:",data_shape)
    feature_columns = [tf.feature_column.numeric_column("x", shape=[data_shape])]
    print(feature_columns)
    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                      hidden_units=[500,500,500],
                                          n_classes=2,
                                          # dropout=0.02,
                                          model_dir="tmp/"+test_data.replace("test","model"),
                                          optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01, l1_regularization_strength=0.001)
                                          )
    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=test_data,
        # na_value='NaN'
        target_dtype=np.int,
        features_dtype=np.float32)



    test_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": np.array(test_set.data)},
        y=np.array(test_set.target),
        num_epochs=1,
        shuffle=False)
    accuracy_score = classifier.evaluate(input_fn=test_input_fn)["accuracy"]

    print("\nTest Accuracy: {0:f}\n".format(accuracy_score))


if predict_only:
    # predict_data = getOptionValue("-predict")

    with open(predict_data) as f:
        for line in f:
            row = line.strip().split(",")
            if len(row) ==2:
                data_shape = int(row[1])
            else:
                break
    print("NUM FEATURES:",data_shape)
    feature_columns = [tf.feature_column.numeric_column("x", shape=[data_shape])]

    predict_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=predict_data,
        # na_value='NaN'
        target_dtype=np.int,
        features_dtype=np.float32)



    predict_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": np.array(predict_set.data)},
        y=np.array(predict_set.target),
        num_epochs=1,
        shuffle=False)
    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                      hidden_units=[500,500,500],
                                          n_classes=2,
                                          # dropout=0.02,
                                          model_dir=modelDir,
                                          optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01, l1_regularization_strength=0.001)
                                          )
    predictions = classifier.predict(input_fn=predict_input_fn)
    # print(predictions.probabilities)
    print("Writing to ",predict_data+"_predictions")
    # print(len(feature_columns),predictions)
    with open(predict_data+"_predictions","w") as out:
        for pred_dict in predictions:
            out.write(str(pred_dict['probabilities'][0])+","+str(pred_dict['probabilities'][1])+"\n")
