#!/usr/bin/env python3

import toxify
from toxify import dfUtil
import os
import sys
import numpy as np
import tensorflow as tf
import pandas as pd
import random
import pickle


data_dir = os.path.dirname(dfUtil.__file__)
def getOptionValue(option):
    optionPos = [i for i, j in enumerate(sys.argv) if j == option][0]
    optionValue = sys.argv[optionPos + 1]
    return optionValue
def dist(x,y):
    return np.sqrt(np.sum((x-y)**2))

if "-t" in sys.argv:
    core_num = getOptionValue("-t")
if "-db" in sys.argv:
    diamond_db = getOptionValue("-db")


train_only = False
test_only = False
predict_only = False
# train_and_test = False

if "--create-features" in sys.argv:
    fasta_dir = getOptionValue("--create-features")
    snake_file = data_dir+"/data/Snakefile"
    # Raptorx_dir = data_dir+"/data/RaptorX_Property_Fast"
    # if not os.path.isfile(Raptorx_dir+"/build_complete.txt"):
    #     os.system("touch "+Raptorx_dir+"/build_complete.txt;cd "+Raptorx_dir+"/source_code;make;cd ../../")
    # bash_command = "snakemake --snakefile "+snake_file+" -d "+fasta_dir+" --config diamond_db="+diamond_db+" --cores "+core_num + " --use-conda --config raptorx_dir="+Raptorx_dir
    bash_command = "snakemake --snakefile "+snake_file+" -d "+fasta_dir+" --cores "+core_num + " --use-conda"
    print(bash_command)
    os.system(bash_command)
if "--make-features" in sys.argv:
    print("making features")
    from math import sqrt
    from joblib import Parallel, delayed
    input_fasta = getOptionValue("--make-features")
    seq_iter = dfUtil.fasta_iter(input_fasta)
    print(Parallel(n_jobs=int(os.cpu_count() *0.8))(delayed(dfUtil.makeCompFeatures)(ff[1]) for ff in seq_iter))

    #out_csv = input_fasta+".csv"

    #with open(out_csv,"w") as out:
    # for ff in input_fasta:
    #     headerStr, seq = ff
    #         # try:
    #     out.write(headerStr.replace("|","_")+",")
    #     out.write(makeCompFeatures(seq)+"\n")

print("CF:",sys.argv[1] == "cloudforest",sys.argv[1])

if "--normalize" in sys.argv:
    input_file = getOptionValue("--normalize")
    df_mean_file = data_dir+"/data/mean.p"
    df_std_file = data_dir+"/data/std.p"
    df_mean = pickle.load( open( df_mean_file, "rb" ) )
    df_std  = pickle.load( open( df_std_file, "rb" ) )
    train_df = pd.read_csv(input_file, skiprows=1, header=None)
    train_data_df = train_df.drop([376],axis=1)
    sub_mean = train_data_df.sub(df_mean,axis=1)
    scaled_df = sub_mean.div(df_std,axis=1)
    scaled_df[376] = train_df[376]
    print(df_mean.shape)
    print(train_df.shape)
    print(train_data_df.shape)
    print(sub_mean.shape)
    print(scaled_df.shape)
    with open(input_file+"norm.csv","w") as out:
        True
    with open(input_file+"norm.csv","a") as out:
        out.write(str(scaled_df.shape[0])+","+str(scaled_df.shape[1]-1)+"\n")
        scaled_df.to_csv(out, header=False,index=False)



if "--prep-train" in sys.argv:
    # should generate six files from list of neg_and_pos = pd.concat([neg_df,pos_df])"all.combined.csv" with relative fasta_paths
    # two test and two train as fm and tf_csv and flat_csv with header column
    fasta_paths = getOptionValue("--prep-train")
    pos_list = []
    neg_list = []
    with open(fasta_paths) as f:
        for line in f:
            row = line.strip().split()
            currentFile = row[1]
            currentDF = pd.read_csv(currentFile, header=None)
            if row[0] == "pos":
                pos_list.append(currentDF)
            if row[0] == "neg":
                neg_list.append(currentDF)
        (train,test) = dfUtil.splitTrain(pos_list,neg_list)
        train.to_csv(fasta_paths+'.train.csv', na_rep='0')
        test.to_csv(fasta_paths+'.test.csv', na_rep='0')

        train.drop('N:feature_0' ,axis=1, inplace=True)
        test.drop('N:feature_0' ,axis=1, inplace=True)
        for key in dfUtil.drop_these:
            train.drop(key ,axis=1, inplace=True)
            test.drop(key ,axis=1, inplace=True)

        with open(fasta_paths+'.train.tf.csv', 'w') as out:
            out.write(str(train.shape[0])+","+str(train.shape[1]-1)+"\n")

        with open(fasta_paths+'.test.tf.csv', 'w') as out:
            out.write(str(test.shape[0])+","+str(test.shape[1]-1)+"\n")
        with open(fasta_paths+'.train.tf.csv', 'a') as f:
            train.to_csv(f, na_rep='0', header=False,index = False)

        with open(fasta_paths+'.test.tf.csv', 'a') as f:
            test.to_csv(f, na_rep='0', header=False,index = False)

        train.insert(0, '.', range(1, 1 + len(train)))
        test.insert(0, '.', range(1, 1 + len(test)))

        train.to_csv(fasta_paths+'.train.fm', na_rep='0',sep='\t',index = False)
        test.to_csv(fasta_paths+'.test.fm', na_rep='0',sep='\t',index = False)
"""
toxclassify.evaluate_dir/neg_euk/all.combined.csv.tf.csvnorm.csv_predictions.csv toxclassify.evaluate_dir/pos/all.combined.csv.tf.csvnorm.csv_predictions.csv toxclassify.evaluate_dir/neg_met/all.combined.csv.tf.csvnorm.csv_predictions.csv
"""

if "--prep-csv" in sys.argv:
    csv_file = getOptionValue("--prep-csv")
    csv_DF = pd.read_csv(csv_file, header=None)
    currentHeaders = list(csv_DF)
    newHeaders = ["N:feature_" + str(header) for header in currentHeaders]
    csv_DF.columns = newHeaders
    csv_DF['C:venom'] = 1
    csv_DF.drop('N:feature_0' ,axis=1, inplace=True)
    for key in dfUtil.drop_these:
        csv_DF.drop(key ,axis=1, inplace=True)
    with open(csv_file+'.tf.csv', 'w') as out:
        out.write(str(csv_DF.shape[0])+","+str(csv_DF.shape[1]-1)+"\n")
    with open(csv_file+'.tf.csv', 'a') as f:
        csv_DF.to_csv(f, na_rep='0', header=False,index = False)
    csv_DF.insert(0, '.', range(1, 1 + len(csv_DF)))
    csv_DF.to_csv(csv_file+'.fm', na_rep='0',sep='\t',index = False)




elif sys.argv[1] == "distance":
    if "-pos" in sys.argv and "-neg" in sys.argv:
        pos_data = getOptionValue("-pos")
        neg_data = getOptionValue("-neg")
        pos_DF = pd.read_csv(pos_data, header=None)
        neg_DF = pd.read_csv(neg_data, header=None)

        df_mean_file = data_dir+"/data/mean.p"
        df_std_file = data_dir+"/data/std.p"
        df_mean = pickle.load( open( df_mean_file, "rb" ) )
        df_std  = pickle.load( open( df_std_file, "rb" ) )


        #############################pos########################################
        currentHeaders = list(pos_DF)
        newHeaders = ["N:feature_" + str(header) for header in currentHeaders]
        pos_DF.columns = newHeaders
        for key in dfUtil.drop_these:
            pos_DF.drop(key ,axis=1, inplace=True)
        neg_DF.columns = newHeaders
        for key in dfUtil.drop_these:
            neg_DF.drop(key ,axis=1, inplace=True)

        pos_DF_noHeaders = pos_DF.drop('N:feature_0' ,axis=1)
        pos_DF_noHeaders.to_csv(pos_data+".noheaders", na_rep='0', header=False,index = False)
        pos_DF_noHeaders = pd.read_csv(pos_data+".noheaders",  header=None)


        sub_mean = pos_DF_noHeaders.sub(df_mean,axis=1)
        scaled_df_pos = sub_mean.div(df_std,axis=1)
        scaled_df_pos["N:feature_0"] = pos_DF["N:feature_0"]



        ################################neg####################################



        neg_DF_noHeaders = neg_DF.drop('N:feature_0' ,axis=1)
        neg_DF_noHeaders.to_csv(neg_data+".noheaders", na_rep='0', header=False,index = False)
        neg_DF_noHeaders = pd.read_csv(neg_data+".noheaders",  header=None)


        sub_mean = neg_DF_noHeaders.sub(df_mean,axis=1)
        scaled_df_neg = sub_mean.div(df_std,axis=1)
        scaled_df_neg["N:feature_0"] = neg_DF["N:feature_0"]

        neg_dict = {}
        for neg_row in range(len(scaled_df_neg)):

            neg_seq = scaled_df_neg.iloc[neg_row][:-1]
            neg_header = scaled_df_neg.iloc[neg_row][-1]
            print(neg_header)

            for pos_row in range(len(scaled_df_pos)):
                pos_seq = scaled_df_pos.iloc[pos_row][:-1]
                pos_header = scaled_df_pos.iloc[pos_row][-1]

                if (neg_header) not in neg_dict:

                    neg_dict[neg_header] = (dist(neg_seq,pos_seq),pos_header)
                elif neg_dict[neg_header][0] >dist(neg_seq,pos_seq):
                    neg_dict[neg_header] = (dist(neg_seq,pos_seq),pos_header)
        #print(neg_dict)
        # print(pd.DataFrame(d).transpose())
        dp = pd.DataFrame(neg_dict).transpose()
        # print(list(dp))
        # print("______________________")
        dp_sort = dp.sort_values(0)
        dp_sort.head(8700).to_csv(neg_data+".nearest")
















elif sys.argv[1] == "cloudforest":
    if "-train" in sys.argv and "-test" in sys.argv:
        training_data = getOptionValue("-train")
        test_data = getOptionValue("-test")
        print("Test:",test_data)
        bash_command = "growforest -train "+ training_data +" -rfpred "+training_data+".forest.sf -oob -nCores 64 -nTrees 1000 -target C:venom -ace 10 -importance "+training_data+".importance.tsv"
        print(bash_command)
        os.system(bash_command)

        if sys.argv[2] == "select":
            print("Now selecting features")
            importance_data = training_data+".importance.tsv"
            importance_dict = {}
            with open(importance_data) as f:
                for line in f:
                    row = line.strip().split()
                    feat = row[1]
                    p_value = float(row[2])
                    # print("p_value, bigger than > 0.001",p_value > 0.001)
                    if p_value > 0.001:
                        importance_dict[feat] = True
            line1 = True
            print("____________________________BEGIN IMPORTANCE____________________________")
            print(importance_dict)
            print("____________________________END IMPORTANCE____________________________")
            with open(training_data+".important.fm","w") as out:
                with open(training_data) as f:
                    keep_index = {}

                    for line in f:
                        line2write  = ""
                        row = line.strip().split()

                        if line1:
                            for i in range(len(row)):
                                if row[i] not in importance_dict:
                                    keep_index[i] = True
                                    line2write += row[i] +"\t"
                            line1 = False

                        else:
                            for i in range(len(row)):
                                # print(i,"in keep_index:",i in keep_index    )
                                if i in keep_index:
                                    line2write += row[i] +"\t"

                        out.write(line2write.strip()+"\n")
            print("opening:",test_data+".important.fm")
            line1 =True
            with open(test_data+".important.fm","w") as out:
                with open(test_data) as f:
                    keep_index = {}

                    for line in f:
                        line2write  = ""
                        row = line.strip().split()

                        if line1:
                            for i in range(len(row)):
                                if row[i] not in importance_dict:
                                    keep_index[i] = True
                                    line2write += row[i] +"\t"
                            line1 = False

                        else:
                            for i in range(len(row)):
                                # print(i,"in keep_index:",i in keep_index    )
                                if i in keep_index:
                                    line2write += row[i] +"\t"

                        out.write(line2write.strip()+"\n")









    else:
        print("please provide training data with -train")
        print("please provide test data with -test")
        sys.exit()







elif sys.argv[1] == "tensorflow":
    if "-train" in sys.argv or "-test" in sys.argv or "-predict"  in sys.argv:

        if "-train" in sys.argv:
            training_data = getOptionValue("-train")
            train_only = True
        if "-test" in sys.argv:
            test_data = getOptionValue("-test")
            test_only = True
        elif "-predict" in sys.argv and "-model" in sys.argv:
            predict_data = getOptionValue("-predict")
            modelDir = getOptionValue("-model")
            predict_only = True

        else:
            print("please provide training data with -train")
            print("please provide test data with -test")
            sys.exit()
# else:
#     print("DO SOMETHING ELSE")
#     sys.exit()


if train_only and test_only:
    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=training_data,
        target_dtype=np.int,
        features_dtype=np.float32)
    with open(training_data) as f:
        for line in f:
            row = line.strip().split(",")
            if len(row) ==2:
                data_shape = int(row[1])
            else:
                break
    print("NUM FEATURES:",data_shape)
    feature_columns = [tf.feature_column.numeric_column("x", shape=[data_shape])]
    print(feature_columns)
    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                      hidden_units=[500,500,500],
                                          n_classes=2,
                                          # dropout=0.02,
                                          model_dir="tmp/"+training_data.replace("train","model"),
                                          optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.00001, l1_regularization_strength=0.001)
                                          )
                                          #Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": np.array(training_set.data)},
        y=np.array(training_set.target),
        num_epochs=20000,
        shuffle=True)
    classifier.train(input_fn=train_input_fn, steps=10000000)

    with open(test_data) as f:
        for line in f:
            row = line.strip().split(",")
            if len(row) ==2:
                data_shape = int(row[1])
            else:
                break
    print("NUM FEATURES:",data_shape)
    feature_columns = [tf.feature_column.numeric_column("x", shape=[data_shape])]
    print(feature_columns)
    # classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
    #                                   hidden_units=[500,500,500],
    #                                       n_classes=2,
    #                                       # dropout=0.02,
    #                                       model_dir="tmp/"+test_data.replace("test","model"),
    #                                       optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01, l1_regularization_strength=0.001)
    #                                       )
    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=test_data,
        # na_value='NaN'
        target_dtype=np.int,
        features_dtype=np.float32)



    test_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": np.array(test_set.data)},
        y=np.array(test_set.target),
        num_epochs=1,
        shuffle=False)
    accuracy_score = classifier.evaluate(input_fn=test_input_fn)["accuracy"]

    print("\nTest Accuracy: {0:f}\n".format(accuracy_score))

# data_shape = training_set.shape[1]
if test_only and not train_only:
    with open(test_data) as f:
        for line in f:
            row = line.strip().split(",")
            if len(row) ==2:
                data_shape = int(row[1])
            else:
                break
    print("NUM FEATURES:",data_shape)
    feature_columns = [tf.feature_column.numeric_column("x", shape=[data_shape])]
    print(feature_columns)
    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                      hidden_units=[500,500,500],
                                          n_classes=2,
                                          # dropout=0.02,
                                          model_dir="tmp/"+test_data.replace("test","model"),
                                          optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.00001, l1_regularization_strength=0.001)
                                          )
    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=test_data,
        # na_value='NaN'
        target_dtype=np.int,
        features_dtype=np.float32)



    test_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": np.array(test_set.data)},
        y=np.array(test_set.target),
        num_epochs=1,
        shuffle=False)
    accuracy_score = classifier.evaluate(input_fn=test_input_fn)["accuracy"]

    print("\nTest Accuracy: {0:f}\n".format(accuracy_score))


if predict_only:
    # predict_data = getOptionValue("-predict")

    with open(predict_data) as f:
        for line in f:
            row = line.strip().split(",")
            if len(row) ==2:
                data_shape = int(row[1])
            else:
                break
    print("NUM FEATURES:",data_shape)
    feature_columns = [tf.feature_column.numeric_column("x", shape=[data_shape])]

    predict_set = tf.contrib.learn.datasets.base.load_csv_with_header(
        filename=predict_data,
        # na_value='NaN'
        target_dtype=np.int,
        features_dtype=np.float32)



    predict_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": np.array(predict_set.data)},
        y=np.array(predict_set.target),
        num_epochs=1,
        shuffle=False)
    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                      hidden_units=[500,500,500],
                                          n_classes=2,
                                          # dropout=0.02,
                                          model_dir=modelDir,
                                          optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01, l1_regularization_strength=0.001)
                                          )
    predictions = classifier.predict(input_fn=predict_input_fn)
    # print(predictions.probabilities)
    print("Writing to ",predict_data+"_predictions")
    # print(len(feature_columns),predictions)
    with open(predict_data+"_predictions","w") as out:
        for pred_dict in predictions:
            out.write(str(pred_dict['probabilities'][0])+","+str(pred_dict['probabilities'][1])+"\n")
